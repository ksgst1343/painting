現在の拡散生成モデルはノイズ加えたデータから元のデータxを予測するのではなく、ノイズεを予測したり、速度vを予測するのが一般的である。しかし、観測空間が高次元のとき、元のデータは低次元多様体に属するのが期待されるのに対し、ノイズや速度は高次元データであり予測が難しい。この論文では改めてデータを予測するのが良いことを示す。

３つの量x, ε, vは、時刻tの時、ノイズ付加した画像z_tに対して、次の関係が成り立つため、いずれかを求めれば、残り２つは解析的に求まる。
z_t = tx + (1 - t) ε
v = x - ε
どの量を予測できるようにニューラルネットワークでモデル化するかの自由度がある。
また、学習の際も損失は真の値に対する、x, ε, vとの二乗誤差を使う差がある。これらより、予測は3通り、損失も3通り、あわせて9通りの設計が可能だが、この中で予測対象を元のデータにした場合のみ、入力次元が大きくなった時でも正確に予測できる。

従来研究では、x予測ではなく、ε、vを予測できるようにモデル化するのが一般的であり、ε、v損失を使って学習していた。
この研究では結論としてx予測を使い、v損失を使うのが良いという結果が得られた。

JiT (Just Image Transfomers）は、元の画像を p x p個の非重複パッチに分割し、各パッチはp x p x 3次元ベクトルとする。各パッチを線形埋め込みし、位置埋め込みを加算。Transformerブロックに通し、出力トークンで線形層でp x p x 3に戻す。
これだけでも、トーカナイザ不要、事前学習や追加学習（GANや知覚損失）なしで、高い生成性能を達成できる。
また、今回の実験では時刻サンプリングをlogit-normal分布からとっている。

特に高次元ではεやvを予測するモデルは学習が破綻する。最も良かったのがxを予測し、損失にはvでの誤差を使う手法である。
また、従来のεやv予測ではモデルの途中の隠れ次元がかなり大きくないと（少なくとも元の入力と同じかそれより）学習できなかったのがx予測では低次元、例えば16でも破綻はせず、極端に小さいボトルネックでも学習が成功する。これは自然画像データが、低次元多様体に存在し、ネットワークが本質的情報を抽出さえすれば、その情報で表現できていることが期待される。汎化性能向上することも期待される。

512x512の高解像度にしたとしてもパッチサイズを32 x 32とすれば、画像トークン数は(512/32)*(512/32)=256であり、通常のTransformerで十分扱える。このように適切なパッチ設計を行えば性能低下しない。

なお、商用などで使われる拡散モデルは画像空間ではなく潜在空間に一度射影してから、そこで拡散モデルを動かす。この場合は低次元であり、かつ潜在変数の分布もノイズと似ているため、εとxの差は殆どでない。
なので潜在拡散モデルを使っている限りは今回の結果は関係ない。

コメント
===
拡散モデルの学習は最初に画像xを予測する研究から、最初のDDPM論文はx予測は画質が悪く、学習が不安定であり、ε予測によって生成品質が大幅に改善されるという実験結果が出たため、それ以降はεやv予測中心だった。

さらには、拡散モデルはスコア（対数確率の勾配）を学習するという発想であり、ガウスノイズの場合は負の予測されたノイズがスコアと一致するため、理論的にも正しいという考え方がされていた。

今、考えればx予測をそのままやれば高ノイズ時は勾配が小さすぎて、低ノイズの時は損失が大きくなり、スケーリングをどのようにすればよいかがわからなかった。ε予測であれば常に正規分布なので学習が安定していた。今回のようにx予測する場合でもv損失を使えば、この損失は時刻tに依存した重み付けが不要で、どの時刻でも勾配スケールが安定するのでそのまま使える。
また、logit-normal分布でtをサンプリングすることで適切に重み付けがされている。

さらに、当時のモデル化能力は小さなUNetを使うため、ノイズを当てる方が簡単で、画像多様体を見つけるほどの能力は持っていなかった。こうしたことからx予測ではなかったのだろう。

NNは低次元多様体を予測させるのが得意という特徴をうまく活かし、従来の常識を覆した良い研究といえる

https://x.com/hillbig/status/1993075344082145601
